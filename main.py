import argparse
import json
from scraper import ZhihuScraper
from utils import create_directories, extract_question_answer_ids
from pdf_generator import PDFGenerator

def load_cookies_from_json(cookie_file):
    """ä»JSONæ–‡ä»¶åŠ è½½cookies"""
    try:
        with open(cookie_file, 'r', encoding='utf-8') as f:
            cookies = json.load(f)
        
        if isinstance(cookies, dict):
            print(f"âœ… æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªcookies")
            return cookies
        else:
            print("âŒ cookiesæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåº”è¯¥æ˜¯JSONå¯¹è±¡æ ¼å¼")
            return {}
            
    except json.JSONDecodeError as e:
        print(f"âŒ JSONæ ¼å¼é”™è¯¯: {e}")
        return {}
    except Exception as e:
        print(f"âŒ åŠ è½½cookieså¤±è´¥: {e}")
        return {}

def main():
    parser = argparse.ArgumentParser(description='çŸ¥ä¹æ–‡ç« çˆ¬å–PDFç”Ÿæˆå™¨')
    parser.add_argument('url', help='çŸ¥ä¹æ–‡ç« URL')
    parser.add_argument('--cookies', '-c', help='cookiesæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºPDFæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    create_directories()
    
    # éªŒè¯URLæ ¼å¼
    question_id, answer_id = extract_question_answer_ids(args.url)
    if not question_id or not answer_id:
        print("âŒ æ— æ•ˆçš„çŸ¥ä¹æ–‡ç« URLæ ¼å¼")
        return
    
    # åŠ è½½cookies
    cookies = {}
    if args.cookies:
        cookies = load_cookies_from_json(args.cookies)
        if not cookies:
            print("âš ï¸  è­¦å‘Š: æœªèƒ½åŠ è½½cookiesï¼Œå°†ä»¥æ¸¸å®¢èº«ä»½è®¿é—®")
    else:
        print("âš ï¸  è­¦å‘Š: æœªæä¾›cookiesæ–‡ä»¶ï¼Œå°†ä»¥æ¸¸å®¢èº«ä»½è®¿é—®")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = ZhihuScraper(cookies)
    
    try:
        print(f"ğŸš€ å¼€å§‹çˆ¬å–æ–‡ç« : {args.url}")
        print(f" é—®é¢˜ID: {question_id}, å›ç­”ID: {answer_id}")
        
        # æå–æ–‡ç« å†…å®¹
        article_data = scraper.extract_article_content(args.url)
        
        if article_data:
            print(f"âœ… æ–‡ç« æ ‡é¢˜: {article_data['title']}")
            print(f"âœ… ä½œè€…: {article_data['author']}")
            print(f"âœ… å›¾ç‰‡æ•°é‡: {len(article_data['images'])}")
            print(f"âœ… å†…å®¹é•¿åº¦: {len(article_data['content'])} å­—ç¬¦")
            
            # ä¿å­˜æ–‡ç« æ•°æ®
            output_file = f"article_data_{article_data['timestamp']}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ–‡ç« æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            
            # ç”ŸæˆPDF
            pdf_generator = PDFGenerator()
            pdf_path = pdf_generator.generate_pdf(article_data, args.output)
            
            if pdf_path:
                print(f"âœ… PDFç”Ÿæˆå®Œæˆ: {pdf_path}")
            else:
                print("âŒ PDFç”Ÿæˆå¤±è´¥")
            
        else:
            print("âŒ æå–æ–‡ç« å†…å®¹å¤±è´¥")
    
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    
    finally:
        scraper.close()

if __name__ == "__main__":
    main() 