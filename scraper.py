import time
import json
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import os
from utils import *
from bs4 import NavigableString

# æ·»åŠ USER_AGENTå¸¸é‡
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

class ZhihuScraper:
    def __init__(self, cookies=None):
        self.cookies = cookies or {}
        self.session = requests.Session()
        self.driver = None
        self.setup_session()
    
    def setup_session(self):
        """è®¾ç½®ä¼šè¯å’Œè¯·æ±‚å¤´"""
        headers = {
            'User-Agent': USER_AGENT,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session.headers.update(headers)
        
        # æ·»åŠ cookies
        for name, value in self.cookies.items():
            self.session.cookies.set(name, value)
    
    def init_driver(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        chrome_options.add_argument('--log-level=3')  # åªæ˜¾ç¤ºè‡´å‘½é”™è¯¯
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument(f'--user-agent={USER_AGENT}')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        
        # æ·»åŠ cookiesåˆ°driver
        self.driver.get("https://www.zhihu.com")
        for name, value in self.cookies.items():
            # è®¾ç½®cookieæ—¶æŒ‡å®šdomainå’Œpathï¼Œç¡®ä¿cookieæ­£ç¡®è®¾ç½®
            cookie_dict = {
                'name': name, 
                'value': value,
                'domain': '.zhihu.com',  # çŸ¥ä¹çš„cookie domain
                'path': '/'
            }
            try:
                self.driver.add_cookie(cookie_dict)
                print(f"âœ… Cookieè®¾ç½®æˆåŠŸ: {name}")
            except Exception as e:
                print(f"âš ï¸ Cookieè®¾ç½®å¤±è´¥ {name}: {e}")
    
    def extract_article_content(self, url):
        """æå–çŸ¥ä¹æ–‡ç« å†…å®¹"""
        try:
            # ä½¿ç”¨Seleniumè·å–åŠ¨æ€å†…å®¹
            if not self.driver:
                self.init_driver()
            
            self.driver.get(url)
            time.sleep(3)  # ç­‰å¾…é¡µé¢åŠ è½½
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "RichText"))
            )
            
            # æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ‡’åŠ è½½çš„å›¾ç‰‡
            self.scroll_page()
            
            # ç­‰å¾…å›¾ç‰‡åŠ è½½
            time.sleep(2)
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # è°ƒè¯•ï¼šæ£€æŸ¥é¡µé¢ä¸­çš„å›¾ç‰‡
            all_images = soup.find_all('img')
            print(f"ğŸ” é¡µé¢ä¸­å…±æ‰¾åˆ° {len(all_images)} ä¸ªå›¾ç‰‡å…ƒç´ ")
            for i, img in enumerate(all_images[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                src = img.get('src', 'No src')
                data_src = img.get('data-src', 'No data-src')
                print(f"  å›¾ç‰‡{i+1}: src={src} | data-src={data_src}")
            
            # æå–æ–‡ç« ä¿¡æ¯
            article_data = self.parse_article(soup, url)
            
            return article_data
            
        except Exception as e:
            print(f"æå–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
            return None
    
    def scroll_page(self):
        """æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ‡’åŠ è½½å†…å®¹"""
        try:
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # æ»šåŠ¨åˆ°é¡µé¢é¡¶éƒ¨
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
        except Exception as e:
            print(f"æ»šåŠ¨é¡µé¢å¤±è´¥: {e}")
    
    def parse_article(self, soup, url):
        """è§£ææ–‡ç« å†…å®¹"""
        article_data = {
            'url': url,
            'title': '',
            'author': '',
            'content': '',
            'images': [],
            'timestamp': get_timestamp()
        }
        
        try:
            print("ğŸ” å¼€å§‹è§£ææ–‡ç« å†…å®¹...")
            
            # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                'h1.QuestionHeader-title',
                '.QuestionHeader h1',
                '.QuestionHeader-title',
                'h1[data-zop-itemid]',
                '.ContentItem-title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    article_data['title'] = title_elem.get_text(strip=True)
                    print(f"âœ… æ‰¾åˆ°æ ‡é¢˜: {article_data['title']}")
                    break
            
            # æå–ä½œè€…ä¿¡æ¯ - å°è¯•å¤šç§é€‰æ‹©å™¨
            author_selectors = [
                '.UserLink',
                '.AnswerItem-authorInfo .UserLink',
                '.ContentItem-meta .UserLink',
                '.AuthorInfo-name',
                '.AnswerItem-authorInfo .AuthorInfo-name'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    article_data['author'] = author_elem.get_text(strip=True)
                    print(f"âœ… æ‰¾åˆ°ä½œè€…: {article_data['author']}")
                    break
            
            # æå–æ–‡ç« å†…å®¹ - å°è¯•å¤šç§é€‰æ‹©å™¨
            content_selectors = [
                '.RichText',
                '.AnswerItem-content .RichText',
                '.ContentItem-content .RichText',
                '.AnswerItem-content',
                '.ContentItem-content'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    print(f"âœ… æ‰¾åˆ°å†…å®¹åŒºåŸŸ: {selector}")
                    article_data['content'], article_data['images'] = self.process_content(content_elem)
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æ›´å®½æ³›çš„é€‰æ‹©å™¨
            if not article_data['content']:
                print("âš ï¸ æœªæ‰¾åˆ°å†…å®¹ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...")
                # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ–‡æœ¬çš„div
                content_divs = soup.find_all('div', class_=lambda x: x and 'RichText' in x)
                if content_divs:
                    content_elem = content_divs[0]
                    article_data['content'], article_data['images'] = self.process_content(content_elem)
            
            print(f"ğŸ“Š è§£æç»“æœ: æ ‡é¢˜={len(article_data['title'])}å­—ç¬¦, ä½œè€…={len(article_data['author'])}å­—ç¬¦, å†…å®¹={len(article_data['content'])}å­—ç¬¦")
            
        except Exception as e:
            print(f"âŒ è§£ææ–‡ç« å¤±è´¥: {e}")
        
        return article_data
    
    def clean_rich_text(self, element):
        """é€’å½’æ¸…ç†çŸ¥ä¹å¯Œæ–‡æœ¬ï¼Œåˆå¹¶span/a/svgç­‰ä¸ºçº¯æ–‡æœ¬"""
        result = ""
        for child in element.children:
            if isinstance(child, NavigableString):
                result += str(child)
            elif child.name in ['span', 'a', 'b', 'strong', 'em']:
                # é€’å½’å¤„ç†ï¼Œä¿ç•™æ–‡æœ¬
                result += self.clean_rich_text(child)
            elif child.name == 'svg':
                # svgä¸€èˆ¬æ˜¯iconï¼Œç›´æ¥ç”¨ç©ºæ ¼æˆ–ç‰¹æ®Šç¬¦å·ä»£æ›¿
                result += " "
            else:
                # å…¶ä»–æ ‡ç­¾é€’å½’å¤„ç†
                result += self.clean_rich_text(child)
        return result

    def process_content(self, content_elem):
        """é€’å½’å¤„ç†çŸ¥ä¹å†…å®¹ï¼Œä¿æŒå›¾æ–‡é¡ºåºï¼Œæ”¶é›†æ‰€æœ‰æœ‰æ•ˆå›¾ç‰‡"""
        content_html = ""
        images = []
        
        def walk(node):
            nonlocal images
            if getattr(node, 'name', None) == 'img':
                src = node.get('src', '')
                # åªå¤„ç†æœ‰æ•ˆå›¾ç‰‡
                if src and src.startswith('http'):
                    img_data = self.process_image(node)
                    if img_data:
                        images.append(img_data)
                        # åœ¨HTMLä¸­æ’å…¥base64å›¾ç‰‡
                        content_type = img_data['content_type']
                        base64_data = img_data['base64_data']
                        return f'<img src="data:{content_type};base64,{base64_data}" alt="{img_data["alt"]}" class="zhihu-image" />'
                return ''  # å¿½ç•¥æ— æ•ˆå›¾ç‰‡
            elif getattr(node, 'name', None) is not None:
                # é€’å½’å¤„ç†å­èŠ‚ç‚¹
                inner = ''.join([walk(child) for child in node.children])
                if node.name in ['p', 'li', 'blockquote', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    return f'<{node.name}>{inner}</{node.name}>'
                elif node.name in ['ul', 'ol']:
                    return f'<{node.name}>{inner}</{node.name}>'
                elif node.name == 'br':
                    return '<br>'
                else:
                    return inner
            else:
                # æ–‡æœ¬èŠ‚ç‚¹
                return str(node)

        content_html = walk(content_elem)
        print(f"âœ… å†…å®¹å¤„ç†å®Œæˆ: {len(content_html)}å­—ç¬¦, {len(images)}å¼ å›¾ç‰‡")
        print(f"ğŸ” å›¾ç‰‡é¡ºåº: {[img['filename'] for img in images]}")
        return content_html, images
    
    def process_paragraph_content(self, p_elem):
        """å¤„ç†æ®µè½å†…å®¹ï¼Œä¿æŒå…¶ä¸­çš„å›¾ç‰‡é¡ºåº"""
        content = ""
        for child in p_elem.children:
            if child.name is None:  # æ–‡æœ¬èŠ‚ç‚¹
                content += str(child)
            elif child.name == 'img':
                # å¤„ç†æ®µè½ä¸­çš„å›¾ç‰‡
                img_data = self.process_image(child)
                if img_data:
                    # åœ¨æ®µè½ä¸­æ’å…¥base64å›¾ç‰‡
                    content_type = img_data['content_type']
                    base64_data = img_data['base64_data']
                    content += f'<img src="data:{content_type};base64,{base64_data}" alt="{img_data["alt"]}" class="zhihu-image" />'
            else:
                # å…¶ä»–å…ƒç´ é€’å½’å¤„ç†
                content += self.clean_rich_text(child)
        return content
    
    def process_list_content(self, list_elem):
        """å¤„ç†åˆ—è¡¨å†…å®¹ï¼Œä¿æŒå…¶ä¸­çš„å›¾ç‰‡é¡ºåº"""
        content = ""
        for li in list_elem.find_all('li', recursive=False):
            li_content = ""
            for child in li.children:
                if child.name is None:  # æ–‡æœ¬èŠ‚ç‚¹
                    li_content += str(child)
                elif child.name == 'img':
                    # å¤„ç†åˆ—è¡¨é¡¹ä¸­çš„å›¾ç‰‡
                    img_data = self.process_image(child)
                    if img_data:
                        content_type = img_data['content_type']
                        base64_data = img_data['base64_data']
                        li_content += f'<img src="data:{content_type};base64,{base64_data}" alt="{img_data["alt"]}" class="zhihu-image" />'
                else:
                    # å…¶ä»–å…ƒç´ é€’å½’å¤„ç†
                    li_content += self.clean_rich_text(child)
            if li_content.strip():
                content += f"<li>{li_content.strip()}</li>"
        return content
    
    def process_image(self, img_elem):
        """å¤„ç†å›¾ç‰‡å…ƒç´ ï¼Œç›´æ¥è·å–base64æ•°æ®"""
        try:
            # å°è¯•å¤šç§å›¾ç‰‡å±æ€§
            src = None
            for attr in ['src', 'data-src', 'data-original', 'data-actualsrc']:
                src = img_elem.get(attr)
                if src:
                    print(f"ğŸ” æ‰¾åˆ°å›¾ç‰‡æº: {attr} = {src}")
                    break
            
            if not src:
                print("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡æº")
                return None
            
            # å¤„ç†ç›¸å¯¹URL
            if src.startswith('//'):
                src = 'https:' + src
            elif src.startswith('/'):
                src = 'https://www.zhihu.com' + src
            elif not src.startswith('http'):
                src = 'https://www.zhihu.com' + src
            
            print(f"ğŸ” å¤„ç†åçš„å›¾ç‰‡URL: {src}")
            
            # ç›´æ¥ä¸‹è½½å›¾ç‰‡åˆ°å†…å­˜
            img_data = self.download_image_to_memory(src)
            if img_data:
                return {
                    'original_url': src,
                    'base64_data': img_data['base64'],
                    'content_type': img_data['content_type'],
                    'filename': img_data['filename'],
                    'alt': img_elem.get('alt', '')
                }
            
        except Exception as e:
            print(f"âŒ å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")
        
        return None
    
    def download_image_to_memory(self, url):
        """ä¸‹è½½å›¾ç‰‡åˆ°å†…å­˜ï¼Œè¿”å›base64æ•°æ®"""
        try:
            print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½å›¾ç‰‡åˆ°å†…å­˜: {url}")
            
            # æ·»åŠ å›¾ç‰‡è¯·æ±‚å¤´
            headers = {
                'User-Agent': USER_AGENT,
                'Referer': 'https://www.zhihu.com/',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            }
            
            response = self.session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', 'image/jpeg')
            if not content_type.startswith('image/'):
                print(f"âš ï¸ éå›¾ç‰‡å†…å®¹: {content_type}")
                return None
            
            # è·å–å›¾ç‰‡æ•°æ®
            image_data = response.content
            
            # è½¬æ¢ä¸ºbase64
            import base64
            base64_data = base64.b64encode(image_data).decode('utf-8')
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆç”¨äºè°ƒè¯•ï¼‰
            filename = generate_filename_from_url(url)
            
            print(f"âœ… å›¾ç‰‡ä¸‹è½½åˆ°å†…å­˜æˆåŠŸ: {filename} ({len(image_data)} bytes)")
            
            return {
                'base64': base64_data,
                'content_type': content_type,
                'filename': filename
            }
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å›¾ç‰‡åˆ°å†…å­˜å¤±è´¥ {url}: {e}")
            return None 
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.driver:
            self.driver.quit() 